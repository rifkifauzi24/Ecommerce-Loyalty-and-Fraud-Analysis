{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rifkifauzi24/Ecommerce-Loyalty-and-Fraud-Analysis/blob/main/Ecommerce_Loyalty_and_Fraud_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "sahilislam007_e_commerce_customer_analytics_loyalty_vs_fraud_path = kagglehub.dataset_download('sahilislam007/e-commerce-customer-analytics-loyalty-vs-fraud')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "BwvNvm6VtduE"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "id": "BwvNvm6VtduE"
    },
    {
      "cell_type": "markdown",
      "id": "7644e5e2",
      "metadata": {
        "id": "7644e5e2"
      },
      "source": [
        "<div style=\"text-align:center; border-radius:15px; padding:15px; color:white; margin:0; font-family: 'Orbitron', sans-serif; background: #2E0249; background: #11001C; box-shadow: 0px 4px 8px rgba(0, 0, 0, 0.3); overflow:hidden; margin-bottom: 1em;\">\n",
        "  <div style=\"font-size:150%; color:#FEE100\"><b>E-commerce Customer Analytics: Loyalty vs Fraud</b></div>\n",
        "  <div>This notebook was created with the help of <a href=\"https://devra.ai/ref/kaggle\" style=\"color:#6666FF\">Devra AI</a></div>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9438059",
      "metadata": {
        "id": "a9438059"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "- [Introduction](#Introduction)\n",
        "- [Data Loading](#Data-Loading)\n",
        "- [Data Cleaning and Preprocessing](#Data-Cleaning-and-Preprocessing)\n",
        "- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
        "- [Predictive Modeling](#Predictive-Modeling)\n",
        "- [Conclusion](#Conclusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47271262",
      "metadata": {
        "id": "47271262"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries and suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Use non-interactive backend if needed\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('Agg')  # Switch backend if only plt is imported\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure inline plotting if running in Jupyter\n",
        "%matplotlib inline\n",
        "\n",
        "# scikit-learn imports for modeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, classification_report\n",
        "\n",
        "sns.set(style='whitegrid')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c59d871e",
      "metadata": {
        "id": "c59d871e"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "There is an intriguing interplay between customer loyalty and fraudulent behavior in the e-commerce space. In this notebook, we explore a synthetic dataset that dives into these dynamics. If you find these insights useful, please consider upvoting this notebook.\n",
        "\n",
        "Our journey begins with loading and cleaning the data, followed by exploratory data analysis and predictive modeling. Let's dive into the analysis and uncover some hidden patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d83399d",
      "metadata": {
        "id": "5d83399d"
      },
      "outputs": [],
      "source": [
        "# Data Loading\n",
        "data_file = 'synthetic_ecommerce_churn_dataset.csv'\n",
        "\n",
        "try:\n",
        "    # Check if file exists in the current directory\n",
        "    if not os.path.exists(data_file):\n",
        "        raise FileNotFoundError(f\"The file {data_file} does not exist in the current directory.\")\n",
        "\n",
        "    df = pd.read_csv(data_file, encoding='ascii', delimiter=',')\n",
        "    print('Dataframe loaded successfully. Here is a preview:')\n",
        "    display(df.head())\n",
        "except FileNotFoundError as e:\n",
        "    print(e)\n",
        "    # This error handling is crucial as many users may encounter a FileNotFoundError if the file path is not set correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e41d53c2",
      "metadata": {
        "id": "e41d53c2"
      },
      "source": [
        "# Data Cleaning and Preprocessing\n",
        "\n",
        "In this section, we address any issues that may be present in the data. Notably, though the dataset specifies the type for each column, some columns such as 'customer_since' are provided as strings even though they represent dates. We will convert these to datetime objects for easier manipulation during analysis. Additionally, we perform basic data cleaning such as handling missing values if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c787c074",
      "metadata": {
        "id": "c787c074"
      },
      "outputs": [],
      "source": [
        "# Data Cleaning and Preprocessing\n",
        "\n",
        "if 'df' in globals():\n",
        "    # Convert 'customer_since' to datetime if it exists\n",
        "    if 'customer_since' in df.columns:\n",
        "        try:\n",
        "            df['customer_since'] = pd.to_datetime(df['customer_since'], errors='coerce')\n",
        "        except Exception as e:\n",
        "            print('Error converting customer_since to datetime:', e)\n",
        "            # It is often useful to use errors='coerce' to ensure non-date values are set as NaT\n",
        "\n",
        "    # Check for missing values\n",
        "    missing_values = df.isnull().sum()\n",
        "    print('Missing values by column:')\n",
        "    print(missing_values)\n",
        "\n",
        "    # A simple approach: drop rows with missing values (could be revisited in future analysis)\n",
        "    df.dropna(inplace=True)\n",
        "    print('\\nData shape after dropping missing values:', df.shape)\n",
        "else:\n",
        "    print('DataFrame is not loaded. Please check the data file path.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d9bd1b9",
      "metadata": {
        "id": "7d9bd1b9"
      },
      "source": [
        "# Exploratory Data Analysis\n",
        "\n",
        "Now, we explore various aspects of the data using several visualization methods. We will use histograms, count plots, box plots, and heatmaps to understand the distributions and relationships in the dataset. Special attention is paid to the interaction between customer loyalty and fraudulent behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9c86e9a",
      "metadata": {
        "id": "c9c86e9a"
      },
      "outputs": [],
      "source": [
        "if 'df' in globals():\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    # Histogram: Distribution of age\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df['age'], kde=True, color='skyblue')\n",
        "    plt.title('Age Distribution')\n",
        "\n",
        "    # Histogram: Distribution of avg_order_value\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.histplot(df['avg_order_value'], kde=True, color='olive')\n",
        "    plt.title('Average Order Value Distribution')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Count plot for categorical variables\n",
        "    plt.figure(figsize=(14, 4))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    sns.countplot(x='gender', data=df, palette='pastel')\n",
        "    plt.title('Gender Distribution')\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    sns.countplot(x='country', data=df, palette='muted')\n",
        "    plt.title('Country Distribution')\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    sns.countplot(x='preferred_category', data=df, palette='bright')\n",
        "    plt.title('Preferred Category')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Correlation Heatmap for numeric features (if 4 or more numeric columns are present)\n",
        "    numeric_df = df.select_dtypes(include=[np.number])\n",
        "    if numeric_df.shape[1] >= 4:\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        corr = numeric_df.corr()\n",
        "        sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "        plt.title('Correlation Heatmap')\n",
        "        plt.show()\n",
        "\n",
        "    # Pair Plot to explore relationships between numeric features\n",
        "    sns.pairplot(numeric_df, diag_kind='hist')\n",
        "    plt.show()\n",
        "else:\n",
        "    print('DataFrame is not loaded. Skipping EDA.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65f83d48",
      "metadata": {
        "id": "65f83d48"
      },
      "source": [
        "# Predictive Modeling\n",
        "\n",
        "In this section, we build a predictor for fraudulent behavior. Our target variable is 'is_fraudulent', and we use several features from the dataset to train a logistic regression model. We will split the data into training and testing sets, train the model, and evaluate its accuracy. Additionally, a confusion matrix and ROC curve will be plotted to assess the model's performance.\n",
        "\n",
        "Note: In case the dataset is too small or the features are noisy, the prediction performance might not be stellar. Regardless, this serves as a baseline model for further tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cad9f7cf",
      "metadata": {
        "id": "cad9f7cf"
      },
      "outputs": [],
      "source": [
        "if 'df' in globals():\n",
        "    # Prepare the data for modeling\n",
        "    # We choose 'is_fraudulent' as the target variable\n",
        "    target = 'is_fraudulent'\n",
        "\n",
        "    # Select a mix of numerical and categorical features for prediction\n",
        "    feature_cols = ['age', 'avg_order_value', 'total_orders', 'last_purchase', 'email_open_rate', 'loyalty_score', 'churn_risk', 'gender', 'country', 'preferred_category']\n",
        "\n",
        "    # Subset the dataframe to only include the features that exist in the dataset\n",
        "    feature_cols = [col for col in feature_cols if col in df.columns]\n",
        "\n",
        "    X = df[feature_cols]\n",
        "    y = df[target]\n",
        "\n",
        "    # One-hot encode categorical features\n",
        "    X = pd.get_dummies(X, columns=['gender', 'country', 'preferred_category'], drop_first=True)\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "    # Initialize and train the logistic regression model\n",
        "    model = LogisticRegression(max_iter=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set and evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'Accuracy of the logistic regression model: {accuracy:.2f}')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.show()\n",
        "\n",
        "    # ROC Curve\n",
        "    y_prob = model.predict_proba(X_test)[:, 1]\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(fpr, tpr, label=f'ROC Curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "    # For further insights, we can display a classification report\n",
        "    print('\\nClassification Report:')\n",
        "    print(classification_report(y_test, y_pred))\n",
        "else:\n",
        "    print('DataFrame is not loaded. Skipping predictive modeling.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ec830f6",
      "metadata": {
        "id": "0ec830f6"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook, we navigated through the e-commerce customer data to study the relationship between loyalty and fraud. Our analysis involved data cleaning, diverse visualizations, and building a logistic regression model to predict fraudulent behavior. While the approach provided some insights, there is certainly room for further analysis such as incorporating more complex models, feature engineering, or exploring time-based trends in customer behavior.\n",
        "\n",
        "Future work could also involve the use of permutation importance to better understand feature contributions and investigating the effect of different categorical variables on fraud detection. The methods and handling of potential file loading errors showcased here are designed to help other notebook creators tackle similar challenges.\n",
        "\n",
        "If you found this notebook useful, please consider upvoting it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57cf3d05",
      "metadata": {
        "id": "57cf3d05"
      },
      "outputs": [],
      "source": [
        "print('Thank you for exploring this analysis. Happy coding!')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}